#拟合效果不好 可能是网络深度不够 或者没有优化？
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import glob
import os

train_image_path = glob.glob('./data_set/dc_2000/train/*/*.jpg')
# glob来提取 用*代表任意

train_image_label = [int(p.split('\\')[1] == 'cat') for p in train_image_path]
# 用列表推导式 int之后的true为1

# print(train_image_label[:5],train_image_label[-5:])

def load_preprosess_image(path, label):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image,channels=3) # 解码
    image = tf.image.resize(image,[360,360])# 不规则的图像进行缩放
    image = tf.image.random_crop(image,[256,256,3])#随机裁剪 写出通道数
    image = tf.image.random_flip_left_right(image)#左右翻转
    image = tf.image.random_flip_up_down(image)#上下翻转
    #brightness contrast 也可以用来加强数据
    image = tf.cast(image,tf.float32)# 转化数据类型
    image = image/255# 归一化
    label = tf.reshape(label,[1])
    return image,label
# tf.image.convert_image_dtype 如果原数据不是float转为float并做归一化

#test上不做图片增强
def load_preprosess_test_image(path, label):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image,channels=3) # 解码
    image = tf.image.resize(image,[256,256])# 不规则的图像进行缩放
    image = tf.cast(image,tf.float32)# 转化数据类型
    image = image/255# 归一化
    label = tf.reshape(label,[1])
    return image,label


train_image_ds = tf.data.Dataset.from_tensor_slices((train_image_path,train_image_label))

AUTOTUNE = tf.data.experimental.AUTOTUNE
#AUTUTUNE根据你的计算机自动使用并行运算
#AUTOTUNE让计算机自己决定

train_image_ds = train_image_ds.map(load_preprosess_image,num_parallel_calls=AUTOTUNE)
#num_parallel_calls 是否用并行运算并选择个数
#map把函数映射在集合上面
#print(train_image_ds)

BATCH_SIZE = 32
train_count = len(train_image_path)
train_image_ds = train_image_ds.shuffle(train_count).batch(BATCH_SIZE).repeat()
train_image_ds = train_image_ds.prefetch(AUTOTUNE)
#加快运行速度 提前读取下一批数据
#生成器迭代



test_image_path = glob.glob('./data_set/dc_2000/test/*/*.jpg')
#构建测试集
test_image_label = [int(p.split('\\')[1] == 'cat') for p in test_image_path]
test_image_ds = tf.data.Dataset.from_tensor_slices((test_image_path,test_image_label))
test_image_ds = test_image_ds.map(load_preprosess_test_image , num_parallel_calls=AUTOTUNE)
test_image_ds = test_image_ds.batch(BATCH_SIZE)
test_image_ds = test_image_ds.prefetch(AUTOTUNE)
#提速
test_count = len(test_image_path)

covn_base = tf.keras.applications.VGG16(weights='imagenet', include_top=False)
#weights是否使用训练好的参数 include_top是否包含最后的全连接层
# covn_base = tf.keras.applications.xception.Xception(
# #     weights='imagenet',
# #     include_top=False,
# #     input_shape=(256,256,3),
# #     pooling='avg'
# # )
#xception网络
model = tf.keras.Sequential()
model.add(covn_base)
model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.Dense(512,activation='relu'))
model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

covn_base.trainable = False
#设置为不可训练

#fine_tune_at =-3
#for layer in covn_base.layers[:fine_tune_at]:
#    layer.trainable = False
model.compile(
    optimizer= tf.keras.optimizers.Adam(lr=0.001),
    loss=tf.keras.losses.binary_crossentropy,
    metrics=['acc']
)

initial_epochs = 12
fine_tune_epochs = 10#微调训练
total_epochs = initial_epochs+fine_tune_epochs

history = model.fit(
    train_image_ds,
    steps_per_epoch=train_count//BATCH_SIZE,
    epochs=15,
    validation_data=test_image_ds,
    validation_steps=test_count//BATCH_SIZE
)

# history = model.fit(
#     train_image_ds,
#     steps_per_epoch=train_count//BATCH_SIZE,
#     epochs=total_epochs,
#     initial_epoch=initial_epochs,#初始训练多少次
#     validation_data=test_image_ds,
#     validation_steps=test_count//BATCH_SIZE
# )