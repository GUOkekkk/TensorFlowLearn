import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
import glob

os.listdir('./data_set/location/annotations/trimaps')

# img = tf.io.read_file('./data_set/location/annotations/trimaps/yorkshire_terrier_99.png')
# img = tf.image.decode_png(img)
# img = tf.squeeze(img)
# # 把维度是1的压缩掉
# plt.imshow(img)
# plt.show()
#
# img1 = tf.io.read_file('./data_set/location/images/yorkshire_terrier_99.jpg')
# img1 = tf.image.decode_jpeg(img1)
# plt.imshow(img1)
# plt.show()

images = glob.glob('./data_set/location/images/*.jpg')
anno = glob.glob('./data_set/location/annotations/trimaps/*.png')
# 要确定是不是一一对应的
np.random.seed(2019)
index = np.random.permutation(len(images))
images = np.array(images)[index]
anno = np.array(anno)[index]
dataset = tf.data.Dataset.from_tensor_slices((images,anno))
test_count = int(len(images)*0.2)
train_count = int(len(images)-test_count)

data_train = dataset.skip(test_count)
data_test = dataset.take(test_count)

def read_jpg(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    return img

def read_png(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_png(img, channels=1)
    return img

def normal_img(input_images,input_anno):
    input_images = tf.cast(input_images,tf.float32)
    input_images = input_images/127.5 - 1
    input_anno -= 1
    return input_images,input_anno

def load_images(images_path,anno_path):
    input_image = read_jpg(images_path)
    input_anno= read_png(anno_path)
    input_image = tf.image.resize(input_image, (224, 224))
    input_anno = tf.image.resize(input_anno, (224, 224))
    return normal_img(input_image, input_anno)

data_train = data_train.map(load_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)
data_test = data_test.map(load_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)

BATCH_SIZE = 8
data_train = data_train.repeat().shuffle(100).batch(BATCH_SIZE)
data_test = data_test.batch(BATCH_SIZE)

for img,anno in data_train.take(1):
    # take一个batch
    plt.subplot(1, 2, 1)
    plt.imshow(tf.keras.preprocessing.image.array_to_img(img[0]))
    plt.subplot(1, 2, 2)
    plt.imshow(tf.keras.preprocessing.image.array_to_img(anno[0]))

conv_base = tf.keras.applications.VGG16(weights='imagenet',
                                        input_shape=(224, 224, 3),
                                        include_top=False)

conv_base.get_layer('block5_conv3').output
sub_model = tf.keras.models.Model(inputs=conv_base.input,
                                  outputs=conv_base.get_layer('block5_conv3').output)
# 创建子模型获取中间网络
# 要创建多输出模型
layer_names = [
    'block5_conv3',
    'block4_conv3',
    'block3_conv3',
    'block5_pool'
]

layers_output = [conv_base.get_layer(name).output for name in layer_names]
# 列表推导式
multi_out_model = tf.keras.models.Model(
    inputs=conv_base.input,
    outputs=layers_output
)

multi_out_model.trainable = False
# 不允许训练
inputs = tf.keras.layers.Input(shape=(224,224,3))
ob5c3, ob4c3, ob3c3, out = multi_out_model(inputs)
# 获取到了中间层的输出

x1 = tf.keras.layers.Conv2DTranspose(512, 3, strides=2, padding='same', activation='relu')(out)
# 反卷积 通过strides来控制大小
x1 = tf.keras.layers.Conv2D(512,3,padding='same',activation='relu')(x1)
x2 = tf.add(x1, ob5c3)
x2 = tf.keras.layers.Conv2DTranspose(512, 3, strides=2, padding='same', activation='relu')(x2)
x2 = tf.keras.layers.Conv2D(512,3,padding='same',activation='relu')(x2)
x3 = tf.add(x2, ob4c3)
x3 = tf.keras.layers.Conv2DTranspose(256, 3, strides=2, padding='same', activation='relu')(x3)
x3 = tf.keras.layers.Conv2D(256,3,padding='same',activation='relu')(x3)
x4 = tf.add(x3, ob3c3)
x5 = tf.keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x4)
x5 = tf.keras.layers.Conv2D(128,3,padding='same',activation='relu')(x5)
prediction = tf.keras.layers.Conv2DTranspose(3, 3, strides=2, padding='same', activation='softmax')(x5)

model = tf.keras.models.Model(
    inputs=inputs,
    outputs=prediction
)
model.compile(
    optimizer='adam',
    loss=tf.losses.sparse_categorical_crossentropy,
    metrics=['acc']
)
model.fit(
    data_train,
    epochs=5,
    steps_per_epoch=train_count//BATCH_SIZE,
    validation_data=data_test,
    validation_steps=test_count//BATCH_SIZE
)

# 绘图查看结果
for image, mask in data_test.take(1):
    pred_mask = model.predict(image)
    pred_mask = tf.argmax(pred_mask,axis=-1)
    pred_mask = pred_mask[...,tf.newaxis]
    
    plt.figure(figsize=(10,10))
    num = 3
    for i in range(num):
        plt.subplot(num, 3, i*num+1)
        plt.imshow(tf.keras.preprocessing.image.array_to_img(image[i]))
        plt.subplot(num, 3, i*num+2)
        plt.imshow(tf.keras.preprocessing.image.array_to_img(mask[i]))
        plt.subplot(num, 3, i*num+3)
        plt.imshow(tf.keras.preprocessing.image.array_to_img(pred_mask[i]))
