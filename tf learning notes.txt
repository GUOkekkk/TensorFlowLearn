tf

常用节点 constant

对话 session #2.0已经取消了

2.0主要使用keras

eager模式直接迭代和直观调试

sequential 有一个输入和一个输出 顺序模型

sigmoid解决二分类问题
loss采用交叉熵binary_crossentropy

softmax解决多分类问题
如果是数字即顺序编码 用sparse_categorical_crossentropy
独热编码用categorical_crossentropy

独热编码 不用数字来做标签 用类似【1，0，0】这种开关

网络容量：可容量的网络参数 越多神经元越多 拟合能力越强 时间长 容易过拟合

超参数：需要自己选择的参数 如学习率或单元层数

增加层数可以大大的提高网络的拟合能力 单层神经元个数不能太小

过拟合：在训练数据上得分高 测试数据上得分低

欠拟合：在训练数据上得分比较低 测试数据上得分相对比较低

dropout：控制过拟合 每次舍去一些网络里的单元 构造不同的数 随机森林
                predict的时候用全部神经元 测试数据的时候也没有dropout
解决过拟合：1. dropout
                    2. 在展平的时候用全局平均池化 也可以降低过拟合
	    3.减小隐藏层的单元数
	    4. 添加正则化参数 kernels_regularztion 正则性衡量函数的光滑性

参数选择原则：先开发过拟合——抑制过拟合：dropout层 正则化 图像增强 最好方法：增加训练数据（交叉验证）
                       最后调整参数 



python -m pip install tensorflow==2.0.0 -i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com

anaconda快速安装


td.data.Dataset 表示一系列的元素 每个元素结构相同 每个元素可以是一个或多个张量 这些tensor称为组件

cnn图像和语音识别

cnn：卷积层：提取数据特征
         非线性层：激活函数
         池化：下采样：提取最有用的特征 简化数据
         全连接层：把输出扁平化变为概率

         卷积：将卷积核应用到某个张量上的所有点上，通过将卷积核在输入的张量上滑动而生成经过滤波处理的张量
                   参数在卷积核上（0，1） 输出一个值 训练卷积核 每一层一个卷积核
                    当一些包含某些特征的图像经过一个卷积核的时候，一些卷积核被激活，输出特定信号


	卷积层：conv2d 把图像变小变厚趋近于多个参数 不同的卷积核对应不同的参数 然后全连接层最终实现到softmax
	非线性变化层：relu/sigmiod/tanh
	池化层：pooling2d 让卷积层的视野扩大
	全连接层：w*x+b


	卷积层：参数：ksize：卷积核的大小；strides：卷积核移动的跨度；padding：边缘填充 'vaild'：不能整除生成图像会小一点    'same'：补充0 使得生成图像形状不变
	
	正则化：让模型较为简单 在损失汉函数中间加入正则化项 如果函数拟合阶数过高就增大了损失函数

	kernals:卷积核 输入的彩色图片 input的通道数为3 则卷积核的通道数为3 但输出output的通道数为1 只和卷积核的个数有关

注：参数的判定 ：（输入个数+1）*输出个数      1为偏置
                              卷积层：原来32个通道 输出到64通道 3*3大小的卷积
                                            参数：（32*9+1）*64

数据标准化：将数据减去平均值使其中心为0 ，然后除以标准差使其标准差为1

数据归一化：除以最大值

批标准化：将分散的数据统一的做法   不仅在将数据输入模型之前对数据做标准化 在网络的每一次变换之后都应该考虑数据标准化
                 解决梯度消失和梯度爆炸的问题 加速收敛 例：sigmoid函数靠近两边 梯度变小
                具有正则化的效果 提高模型的泛化能力 允许更高的学习效率从而加速收敛
	有助于梯度传播 允许更深层的网络
	训练中会记录下下线性变换 而得到原来的数据 然后可以求出学习速率以至改变权值
	预测的均值和方差也来自训练集的记录 training参数Boolean类型来判断预测或者训练
	放在激活函数之后效果更好（论文提出在激活前面好一点）

tensorboard: 读取的是进程 要用回调函数 在cmd里面打开 tensorboard --logdir 路径 或者一层一层进入
	      界面：scalars 标量：acc，loss
		graph:整个网络的架构
		distribution/histogram：显示权重的变化
		(base) C:\Users\gkbb>e:

		(base) E:\>cd pythonwork

		(base) E:\pythonwork>cd learning_program

		(base) E:\pythonwork\learning_program>tensorboard --logdir logs
	  cmd打开时才能连接
	  记录自定义标量 创建文件编写器 tf.summary.create_file_writer() 用scalar记录 再把参数回调给fit



Eager模式：添加自定义的神经网络+自定义循环
	   命令式编程环境 我们可以立即评估操作产生的结果 无需构建计算图即搭好整个神经网络
	  tensorflow 的交互模式 
	  2.0 默认使用 提供了一个灵活的研究和实验机器学习平台，提供：直观的界面——自然的构建代码并使用Python数据结构 快速迭代小型模型和小型数据
	  更容易调试——在交互式环境中直接检查，运行模型，测试变化。这个过程中代码会即时错误报告。
	  自然控制流——eager模式下使用python控制流而不是图控制流简化了动态模型的创建 热切执行支持大多数Tensorflow操作和GPU加速
	  eager模式下,tf.Tensor对象引用具体的值而不是计算图中节点的符号
	  用print输出值
	  理解：就是把图计算变成了正常的python计算 让tf这个框架和普通的python一样



Tensor：高维数组 有shape 


自动微分：用tape来记录运算过程来计算微分
	w = tf.Variable([[1.0]])
	with tf.GradientTape() as t:
    	      loss = w*w
	#上下文管理器 用梯度磁带记录
	grad = t.gradient(loss,w)
	#哪个函数对哪个变量微分

	自动追踪运算


tf.keras.metrics： 计算模块


数据增强：对数据进行变化来的到新的数据 减小过拟合
	例如在cnn中对图片进行反转 截取

VGG-16 : 小卷积核 深层 卷积步长为1 变成黑白图片  缺点：网络架构太大 

预训练网络：在一个足够大且通用的原始数据集上训练好的网络 

迁移学习：用预训练网络对小数据问题进行预测
	训练好的卷积基+自定义的分类器FC层


IMAGENET：一个很大的图片数据库 1000个图片类型的比赛

微调：冻结模型的底部的卷积层 共同训练新添加的分类器层和顶部部分卷积层     ps：底部卷积层是一些通用的特征 注意要先训练好分类器

图像处理任务：分类 定位 语义分割 实例分割（同一个物体的不同个体） 目标检测

图像定位：需要输出四个数字 (x,y,w,h)图像某一点的坐标和图像的宽度和高度
	先卷积——>全连接层——>两种激活（softmax，L2loss回归问题来定位）

图像定位的优化：缺点：回归位置不准确，泛化能力不好，算法只能预测单个实例
	          iou：交并比来衡量

Graph Execution模式：图运算模式 建立session 类似eager模式

图像语义分割：预测出图像中每一个像素的类的标签（不是实例分割 没有对个数分割）
	       FCN：全卷积神经网络（基石） 全连接层（丢失了空间信息）把全连接层变成卷积层 再上采样还原 通道数：n(目标类别数)+1(背景)
		 缺点：结果不够精细 对细节不够敏感 没有考虑到像素之间的关系 缺乏空间一致性
	       UNET
上采样：1.插值法 2.反池化 3.反卷积:(转置卷积):通过训练放大图片

FCN的跳接结构：一开始视野小 用pool层去扩大视野
	           用SKIP层将最后一层的预测和浅层的细节预测结合起来 将底层的预测进行上采样与前面层的预测结合 再将其上采样与之前的结合(用tf.add方法)

Unet：语义分割应用最广，能在更少的训练图像中学习(医疗中常用) 医疗影像语义分割的基础网络结构
           前半部分特征提取 后半部分上采样 编码器-解码器结构
	   

RNN循环神经网络：一个序列的当前输出与前面的输出也有关 网络会对前面的信息进行记忆并应用到当前输出的计算中 
		即隐藏层之间的节点不再是无连接的而是有链接的 隐藏层的输入包括输入层和上一时刻隐藏层的输出
		ps:有可能出现梯度的消失或者爆炸

LSTM网络：long short term是RNN的一种特殊类型 可以学习长期依赖信息 可以绕过循环直接传递到最后
	   keys：拥有一个固定权值为1的自连接和一个线性激活函数 局部偏导为1 不会引起梯度的消失和爆炸
	              LSTM通过门对通过的信息进行控制 可以让信息不通过 完全通过 通过一部份
	   ps:变形GRU门循环单元 结构更简单 有一个更新门 更新门决定了内部的状态与输入state状态融合比例
	        比LSTM结构简单 计算少 效果相当


Kaggle:不需要布置环境 可以使用服务器级别GPU