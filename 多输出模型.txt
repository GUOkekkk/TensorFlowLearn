import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import random
import pathlib

data_dir = './data_set/moc'
data_root = pathlib.Path(data_dir)
#以path为根目录的

all_image_paths = list(data_root.glob('*/*'))
image_count = len(all_image_paths)


all_image_paths = [str(path) for path in all_image_paths]
random.shuffle(all_image_paths)
#乱序

label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())

color_label_names = set(name.split('_')[0] for name in label_names)
class_label_names = set(name.split('_')[1] for name in label_names)

color_label_to_index = dict((name,index) for index,name in enumerate(color_label_names))
class_label_to_index = dict((name,index) for index,name in enumerate(class_label_names))
#enumerate 生成器迭代器

all_image_labels = [pathlib.Path(path).parent.name for path in all_image_paths]
color_labels = [color_label_to_index[label.split('_')[0]] for label in all_image_labels]
class_labels = [class_label_to_index[label.split('_')[1]] for label in all_image_labels]

def load_and_preprocess_image(path):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image,channels=3)
    image = tf.image.resize(image,[224,224])
    image = tf.cast(image,tf.float32)
    image = image/255.0
    #归一化
    image = 2*image-1
    #[-1,1]之间
    return image

image_path = all_image_paths[0]
label = all_image_labels[0]

# plt.imshow((load_and_preprocess_image(image_path)+1)/2)
# plt.grid(False)
# plt.xlabel(label)
# plt.show()

path_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)
AUTOTUNE = tf.data.experimental.AUTOTUNE
image_ds = path_ds.map(load_and_preprocess_image , num_parallel_calls=AUTOTUNE)
label_ds = tf.data.Dataset.from_tensor_slices((color_labels,class_labels))

image_label_ds = tf.data.Dataset.zip((image_ds,label_ds))

test_count = int(image_count*0.2)
train_count = image_count - test_count

train_data = image_label_ds.skip(test_count)
test_data = image_label_ds.take(test_count)

BATCH_SIZE = 32

train_data = train_data.shuffle(buffer_size=train_count).repeat(-1)
train_data = train_data.batch(BATCH_SIZE)
train_data = train_data.prefetch(buffer_size=AUTOTUNE)

test_data = test_data.batch(BATCH_SIZE)

#使用预训练网络

inputs = tf.keras.Input(shape=(224,224,3))
mobile_net = tf.keras.applications.MobileNetV2(input_shape = (224,224,3),include_top = False)
#小型网络 这里好像没有下载好 一直在报错

x = mobile_net(inputs)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
#定义不同的Dense

x1 = tf.keras.layers.Dense(1024,activation='relu')(x)
ouput_color = tf.keras.layers.Dense(3,activation='softmax',name='oupot_color')(x1)

x2 = tf.keras.layers.Dense(1024,activation='relu')(x)
ouput_class = tf.keras.layers.Dense(4,activation='softmax',name='ouput_class')(x2)
#为其设置name，便于建立字典，储存不同的loss

model = tf.keras.Model(
    inputs = inputs,
    outputs = [ouput_color,ouput_class]
)

model.compile(
    optimizer='adam',
    loss={'ouput_color':'sparse_categorical_crossentropy',
          'ouput_class':'sparse_categorical_crossentropy'},
    metrics=['acc']
)

train_steps = train_count//BATCH_SIZE
test_steps = test_count//BATCH_SIZE

model.fit(
    train_data,
    epochs=15,
    steps_per_epoch=train_steps,
    validation_data=test_data,
    validation_steps=test_steps
)
#一定要确保输入的维度和要求的维度一样

